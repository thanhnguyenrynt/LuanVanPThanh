@article{yang2016classification,
  title   = {Classification of trash for recyclability status},
  author  = {Yang, Mindy and Thung, Gary},
  journal = {CS229 project report},
  volume  = {2016},
  number  = {1},
  pages   = {3},
  year    = {2016}
}

@inproceedings{shah2022method,
  title        = {A method for waste segregation using convolutional neural networks},
  author       = {Shah, Jash and Kamat, Sagar},
  booktitle    = {2022 Second International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)},
  pages        = {1--5},
  year         = {2022},
  organization = {IEEE}
}

@article{ahmad2020intelligent,
  title     = {Intelligent fusion of deep features for improved waste classification},
  author    = {Ahmad, Kashif and Khan, Khalil and Al-Fuqaha, Ala},
  journal   = {IEEE access},
  volume    = {8},
  pages     = {96495--96504},
  year      = {2020},
  publisher = {IEEE}
}

@article{nhung2021nghien,
  title   = {Nghi{\^e}n cứu m{\^o} h{\`\i}nh ph{\'a}t hiện r{\'a}c thải nhựa ven biển sử dụng ảnh m{\'a}y bay kh{\^o}ng người l{\'a}i v{\`a} mạng nơ-ron t{\'\i}ch chập s{\^a}u},
  author  = {Nhung, {\DJ}ỗ Thị and My, Nguyễn Thị Diễm and Mạnh, Phạm V{\u{a}}n and {\DJ}{\^o}ng, Phạm V{\~u} and Th{\`a}nh, B{\`u}i Quang and Tuấn, Nghi{\^e}m V{\u{a}}n and Hải, Phạm Minh},
  journal = {Tạp ch{\'\i} Khoa học {\DJ}o {\dj}ạc v{\`a} Bản {\dj}ồ},
  number  = {49},
  pages   = {21--29},
  year    = {2021}
}

@article{simonyan2014very,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014}
}

@article{Papert1966TheSV,
  title  = {The Summer Vision Project},
  author = {Seymour Papert},
  year   = {1966},
  url    = {https://api.semanticscholar.org/CorpusID:60684578}
}

@article{agatonovic2000basic,
  title     = {Basic concepts of artificial neural network (ANN) modeling and its application in pharmaceutical research},
  author    = {Agatonovic-Kustrin, S and Beresford, Rosemary},
  journal   = {Journal of pharmaceutical and biomedical analysis},
  volume    = {22},
  number    = {5},
  pages     = {717--727},
  year      = {2000},
  publisher = {Elsevier}
}

@article{hubel1962receptive,
  title     = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
  author    = {Hubel, David H and Wiesel, Torsten N},
  journal   = {The Journal of physiology},
  volume    = {160},
  number    = {1},
  pages     = {106},
  year      = {1962},
  publisher = {Wiley-Blackwell}
}

@article{lecun1998gradient,
  title     = {Gradient-based learning applied to document recognition},
  author    = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal   = {Proceedings of the IEEE},
  volume    = {86},
  number    = {11},
  pages     = {2278--2324},
  year      = {1998},
  publisher = {Ieee}
}

@inproceedings{nair2010rectified,
  title     = {Rectified linear units improve restricted boltzmann machines},
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages     = {807--814},
  year      = {2010}
}

@article{caruana2000overfitting,
  title   = {Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping},
  author  = {Caruana, Rich and Lawrence, Steve and Giles, C},
  journal = {Advances in neural information processing systems},
  volume  = {13},
  year    = {2000}
}

@article{krizhevsky2012imagenet,
  title   = {Imagenet classification with deep convolutional neural networks},
  author  = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal = {Advances in neural information processing systems},
  volume  = {25},
  year    = {2012}
}

@inproceedings{dalal2005histograms,
  title        = {Histograms of oriented gradients for human detection},
  author       = {Dalal, Navneet and Triggs, Bill},
  booktitle    = {2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)},
  volume       = {1},
  pages        = {886--893},
  year         = {2005},
  organization = {Ieee}
}

@misc{girshick2014rich,
  title         = {Rich feature hierarchies for accurate object detection and semantic segmentation},
  author        = {Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
  year          = {2014},
  eprint        = {1311.2524},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{uijlings2013selective,
  title     = {Selective search for object recognition},
  author    = {Uijlings, Jasper RR and Van De Sande, Koen EA and Gevers, Theo and Smeulders, Arnold WM},
  journal   = {International journal of computer vision},
  volume    = {104},
  pages     = {154--171},
  year      = {2013},
  publisher = {Springer}
}

@misc{girshick2015fast,
  title         = {Fast R-CNN},
  author        = {Ross Girshick},
  year          = {2015},
  eprint        = {1504.08083},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@inbook{He_2014,
  title     = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
  isbn      = {9783319105789},
  issn      = {1611-3349},
  url       = {http://dx.doi.org/10.1007/978-3-319-10578-9_23},
  doi       = {10.1007/978-3-319-10578-9_23},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year      = {2014},
  pages     = {346–361}
}

@misc{ren2016faster,
  title         = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
  author        = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
  year          = {2016},
  eprint        = {1506.01497},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{testtimefastrcnn,
  author       = {Fei Fei Li & Justin Johnson & Serena Yeung},
  title        = {Detection and Segmentation},
  howpublished = {\url{https://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf}},
  year         = {2017}
}

@misc{redmon2016look,
  title         = {You Only Look Once: Unified, Real-Time Object Detection},
  author        = {Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  year          = {2016},
  eprint        = {1506.02640},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{redmon2016yolo9000,
  title         = {YOLO9000: Better, Faster, Stronger},
  author        = {Joseph Redmon and Ali Farhadi},
  year          = {2016},
  eprint        = {1612.08242},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@unknown{zhang2023,
  author = {Zhang, Youshan},
  year   = {2023},
  month  = {03},
  pages  = {},
  title  = {Stall Number Detection of Cow Teats Key Frames}
}

@misc{redmon2018yolov3,
  title         = {YOLOv3: An Incremental Improvement},
  author        = {Joseph Redmon and Ali Farhadi},
  year          = {2018},
  eprint        = {1804.02767},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{lin2017feature,
  title         = {Feature Pyramid Networks for Object Detection},
  author        = {Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
  year          = {2017},
  eprint        = {1612.03144},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{bochkovskiy2020yolov4,
  title         = {YOLOv4: Optimal Speed and Accuracy of Object Detection},
  author        = {Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
  year          = {2020},
  eprint        = {2004.10934},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{simonyan2015deep,
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author        = {Karen Simonyan and Andrew Zisserman},
  year          = {2015},
  eprint        = {1409.1556},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{he2015deep,
  title         = {Deep Residual Learning for Image Recognition},
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year          = {2015},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{xie2017aggregated,
  title         = {Aggregated Residual Transformations for Deep Neural Networks},
  author        = {Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
  year          = {2017},
  eprint        = {1611.05431},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{huang2018densely,
  title         = {Densely Connected Convolutional Networks},
  author        = {Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
  year          = {2018},
  eprint        = {1608.06993},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{wang2019cspnet,
  title         = {CSPNet: A New Backbone that can Enhance Learning Capability of CNN},
  author        = {Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh},
  year          = {2019},
  eprint        = {1911.11929},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{liu2018path,
  title         = {Path Aggregation Network for Instance Segmentation},
  author        = {Shu Liu and Lu Qi and Haifang Qin and Jianping Shi and Jiaya Jia},
  year          = {2018},
  eprint        = {1803.01534},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{woo2018cbam,
  title         = {CBAM: Convolutional Block Attention Module},
  author        = {Sanghyun Woo and Jongchan Park and Joon-Young Lee and In So Kweon},
  year          = {2018},
  eprint        = {1807.06521},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{s22020464,
  author         = {Nepal, Upesh and Eslamiat, Hossein},
  title          = {Comparing YOLOv3, YOLOv4 and YOLOv5 for Autonomous Landing Spot Detection in Faulty UAVs},
  journal        = {Sensors},
  volume         = {22},
  year           = {2022},
  number         = {2},
  article-number = {464},
  url            = {https://www.mdpi.com/1424-8220/22/2/464},
  pubmedid       = {35062425},
  issn           = {1424-8220},
  abstract       = {In-flight system failure is one of the major safety concerns in the operation of unmanned aerial vehicles (UAVs) in urban environments. To address this concern, a safety framework consisting of following three main tasks can be utilized: (1) Monitoring health of the UAV and detecting failures, (2) Finding potential safe landing spots in case a critical failure is detected in step 1, and (3) Steering the UAV to a safe landing spot found in step 2. In this paper, we specifically look at the second task, where we investigate the feasibility of utilizing object detection methods to spot safe landing spots in case the UAV suffers an in-flight failure. Particularly, we investigate different versions of the YOLO objection detection method and compare their performances for the specific application of detecting a safe landing location for a UAV that has suffered an in-flight failure. We compare the performance of YOLOv3, YOLOv4, and YOLOv5l while training them by a large aerial image dataset called DOTA in a Personal Computer (PC) and also a Companion Computer (CC). We plan to use the chosen algorithm on a CC that can be attached to a UAV, and the PC is used to verify the trends that we see between the algorithms on the CC. We confirm the feasibility of utilizing these algorithms for effective emergency landing spot detection and report their accuracy and speed for that specific application. Our investigation also shows that the YOLOv5l algorithm outperforms YOLOv4 and YOLOv3 in terms of accuracy of detection while maintaining a slightly slower inference speed.},
  doi            = {10.3390/s22020464}
}

@misc{li2022yolov6,
  title         = {YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications},
  author        = {Chuyi Li and Lulu Li and Hongliang Jiang and Kaiheng Weng and Yifei Geng and Liang Li and Zaidan Ke and Qingyuan Li and Meng Cheng and Weiqiang Nie and Yiduo Li and Bo Zhang and Yufei Liang and Linyuan Zhou and Xiaoming Xu and Xiangxiang Chu and Xiaoming Wei and Xiaolin Wei},
  year          = {2022},
  eprint        = {2209.02976},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{yolov72023,
  author = {Al-refai, Ghaith and ElMoaqet, Hisham and Ryalat, Mutaz and Al-refai, Mohammed},
  year   = {2023},
  month  = {09},
  pages  = {},
  title  = {Object Detection In Low-Light Environment Using YOLOv7},
  doi    = {10.21203/rs.3.rs-3365905/v1}
}

@misc{wang2022yolov7,
  title         = {YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  author        = {Chien-Yao Wang and Alexey Bochkovskiy and Hong-Yuan Mark Liao},
  year          = {2022},
  eprint        = {2207.02696},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}


@inbook{Liu_2016,
  title     = {SSD: Single Shot MultiBox Detector},
  isbn      = {9783319464480},
  issn      = {1611-3349},
  url       = {http://dx.doi.org/10.1007/978-3-319-46448-0_2},
  doi       = {10.1007/978-3-319-46448-0_2},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  author    = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year      = {2016},
  pages     = {21–37}
}

@misc{retinaVn,
  author = {Duy, Nguyễn and Tiến, Đỗ and Duc, Thanh and Tín, Huỳnh and Duy, Lê},
  year   = {2019},
  month  = {10},
  pages  = {},
  title  = {ĐÁNH GIÁ CÁC PHƯƠNG PHÁP DỰA TRÊN DEEP LEARNING CHO BÀI TOÁN PHÁT HIỆN LOGO},
  doi    = {10.15625/vap.2019.00017}
}



@misc{lin2018focal,
  title         = {Focal Loss for Dense Object Detection},
  author        = {Tsung-Yi Lin and Priya Goyal and Ross Girshick and Kaiming He and Piotr Dollár},
  year          = {2018},
  eprint        = {1708.02002},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{proença2020taco,
  title         = {TACO: Trash Annotations in Context for Litter Detection},
  author        = {Pedro F Proença and Pedro Simões},
  year          = {2020},
  eprint        = {2003.06975},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{visualization-tools-for-taco-dataset,
  title        = { Visualization Tools for TACO Dataset },
  type         = { Computer Vision Tools },
  author       = { Dataset Ninja },
  howpublished = { \url{ https://datasetninja.com/taco } },
  url          = { https://datasetninja.com/taco },
  journal      = { Dataset Ninja },
  publisher    = { Dataset Ninja },
  year         = { 2024 },
  month        = { may },
  note         = { visited on 2024-05-11 }
}

@misc{lee2019energy,
  title         = {An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection},
  author        = {Youngwan Lee and Joong-won Hwang and Sangrok Lee and Yuseok Bae and Jongyoul Park},
  year          = {2019},
  eprint        = {1904.09730},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{Majchrowska_2022,
  title     = {Deep learning-based waste detection in natural and urban environments},
  volume    = {138},
  issn      = {0956-053X},
  url       = {http://dx.doi.org/10.1016/j.wasman.2021.12.001},
  doi       = {10.1016/j.wasman.2021.12.001},
  journal   = {Waste Management},
  publisher = {Elsevier BV},
  author    = {Majchrowska, Sylwia and Mikołajczyk, Agnieszka and Ferlin, Maria and Klawikowska, Zuzanna and Plantykow, Marta A. and Kwasigroch, Arkadiusz and Majek, Karol},
  year      = {2022},
  month     = feb,
  pages     = {274–284}
}
   
@article{cachuayol7,
  title        = {10. NGHIÊN CỨU VÀ ỨNG DỤNG THUẬT TOÁN YOLOV7 ĐỂ PHÂN LOẠI CÀ CHUA},
  url          = {https://tapchikhtnmt.hunre.edu.vn/index.php/tapchikhtnmt/article/view/569},
  doi          = {10.63064/khtnmt.2024.569},
  abstractnote = {&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Hiện nay, &amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;t&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;rí tuệ nhân tạo và &amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;h&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;ọc máy đang ngày càng phát triển và được ứng dụng rộng rãi trong nhiều lĩnh vực trong đời sống. Trí tuệ nhân tạo có khả năng xử lý dữ liệu khổng lồ, học hỏi từ dữ liệu và đưa ra quyết định một cách tự động. Trong quá trình nghiên cứu, nhóm đã ứng dụng nhận diện cà chua bằng thị giác máy tính thông qua thuật toán YOLOv7&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;. T&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;ừ đó đánh giá, phân tích dựa trên dữ liệu cà chua thu thập được.&amp;lt;/em&amp;gt; &amp;lt;em&amp;gt;M&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;ô hình nhận diện quả cà chua đã đạt được độ chính xác 93&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;,&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;3&amp;lt;/em&amp;gt; &amp;lt;em&amp;gt;% đối với quả bình thường và đạt 89&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;,&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;1&amp;lt;/em&amp;gt; &amp;lt;em&amp;gt;%&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt; với quả hỏng trên tập dữ liệu thử nghiệm.&amp;lt;/em&amp;gt; &amp;lt;em&amp;gt;Từ kết quả nghiên cứu có thể phân loại được cà chua một cách chính xác nhất nhằm tăng sản lượng, chất lượng sản phẩm trong lĩnh vực nông nghiệp Việt Nam&amp;lt;/em&amp;gt;&amp;lt;em&amp;gt;.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;},
  number       = {50},
  journal      = {Tạp chí khoa học Tài nguyên và Môi trường},
  author       = {Nguyễn Văn, Mạnh and Lê Văn, An and Trần Đức, Lương and Nguyễn Tuấn, Tú and Nguyễn Thị Thanh, Huyền and Ngô Thị Bích, Thúy and Nguyễn Thái, Cường},
  year         = {2024},
  month        = {tháng 3},
  pages        = {102–112}
}
@misc{sandler2019mobilenetv2,
  title         = {MobileNetV2: Inverted Residuals and Linear Bottlenecks},
  author        = {Mark Sandler and Andrew Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
  year          = {2019},
  eprint        = {1801.04381},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@software{YOLOv8,
  author  = {Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing},
  license = {AGPL-3.0},
  month   = {01},
  title   = {{Ultralytics YOLO}},
  url     = {https://github.com/ultralytics/ultralytics},
  version = {8.0.0},
  year    = {2023}
}


@inproceedings{9122693,
  author    = {Carolis, Berardina De and Ladogana, Francesco and Macchiarulo, Nicola},
  booktitle = {2020 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS)},
  title     = {YOLO TrashNet: Garbage Detection in Video Streams},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1-7},
  keywords  = {Waste management;video stream analysis;computer vision;deep learning},
  doi       = {10.1109/EAIS48028.2020.9122693}
}

@inproceedings{8793975,
  author    = {Fulton, Michael and Hong, Jungseok and Islam, Md Jahidul and Sattar, Junaed},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  title     = {Robotic Detection of Marine Litter Using Deep Visual Detection Models},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {5752-5758},
  keywords  = {Plastics;Training;Oceans;Data models;Object detection;Visualization;Biological system modeling},
  doi       = {10.1109/ICRA.2019.8793975}
}

@Article{info14120633,
AUTHOR = {Single, Sam and Iranmanesh, Saeid and Raad, Raad},
TITLE = {RealWaste: A Novel Real-Life Data Set for Landfill Waste Classification Using Deep Learning},
JOURNAL = {Information},
VOLUME = {14},
YEAR = {2023},
NUMBER = {12},
ARTICLE-NUMBER = {633},
URL = {https://www.mdpi.com/2078-2489/14/12/633},
ISSN = {2078-2489},
ABSTRACT = {The accurate classification of landfill waste diversion plays a critical role in efficient waste management practices. Traditional approaches, such as visual inspection, weighing and volume measurement, and manual sorting, have been widely used but suffer from subjectivity, scalability, and labour requirements. In contrast, machine learning approaches, particularly Convolutional Neural Networks (CNN), have emerged as powerful deep learning models for waste detection and classification. This paper analyses VGG-16, InceptionResNetV2, DenseNet121, Inception V3, and MobileNetV2 models to classify real-life waste when trained on pristine and unadulterated materials, versus samples collected at a landfill site. When training on DiversionNet, the unadulterated material dataset with labels required for landfill modelling, classification accuracy was limited to 49.69% in the real environment. Using real-world samples in the newly formed RealWaste dataset showed that practical applications for deep learning in waste classification are possible, with Inception V3 reaching 89.19% classification accuracy on the full spectrum of labels required for accurate modelling.},
DOI = {10.3390/info14120633}
}
